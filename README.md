The “2006” and “2007” year datasets obtained from the Harvard Website (https://doi.org/10.7910/DVN/HG7NV7) were used for this project. Before conducting an in-depth analysis, the data was thoroughly cleaned to ensure accuracy and reliability. This involved removing duplicates, addressing missing values, normalizing data formats, and correcting errors and outliers. By meticulously preparing the data, we ensured the subsequent analysis was based on high-quality, reliable information.

Both Python (using Jupyter Notebooks) and R (using R Studio) programming languages were used to visualize the data, addressing the following research questions:

(a) What are the best times and days of the week to minimise delays each year?

(b) Evaluate whether older planes suffer more delays on a year-to-year basis.

(c) For each year, fit a logistic regression model for the probability of diverted US flights using as many features as possible from attributes of the departure date, the scheduled departure and arrival times, the coordinates and distance between departure and planned arrival airports, and the carrier. Visualize the coefficients across years.
